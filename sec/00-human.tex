%!TEX root = ../main.tex
\section{Optimized Goodcore with Imputation-in-the-loop}
\label{sec:human}


As discussed above, it is rather expensive to directly compute all the $K$ tuples in the coreset.  Hence, in this section, we propose to involve the imputation-in-the-loop mechanism that asks the human, \ie Case (7), or automatic method, \ie Case (8) to impute these missing values iteratively while they are generated by Algorithm~\ref{alg:framework}.


The advantages of this optimization are two-fold. First, with more and more missing values being imputed, the number of possible worlds is greatly reduced, which reduces the machine cost a lot. Second, for human-in-the-loop imputation, it allows us to  gradually impute the tuples accurately, and thus the coreset score computation can be more and more accurate, which produces a better coreset.

 %At a high level, there are two advantages of the human-in-the-loop strategy. On the one hand, in each iteration, the number of possible world is greatly reduced, and thus the time complexity is much lower. On the other hand, as the human has imputed values in previous iterations,  the coreset score computation will be more accurate for the following iterations. That is, the human-in-the-loop strategy continuously improves the accuracy of measuring a coreset using the score, which apparently helps to generate a  better coreset. Next, we  discuss how to  impute one tuple per iteration and impute a small batch of  tuples per iteration respectively.

\input{sec/algo-one}

	%\nllabel{craig:sample}\nllabel{alg:maxmulti} 	 \nllabel{alg:add2} \nllabel{alg:if}	\nllabel{alg:oracle}

\subsection{One Tuple Each Iteration}~\label{subsec:one}



In fact, we can just slightly modify Algorithm~\ref{alg:framework} to  achieve the imputation-in-the-loop strategy.
To be specific,  in the first loop, we will iteratively impute the tuple   once an incomplete  tuple $t^*$ is computed by \ours, rather than conducting the imputation after $K$ tuples are computed, as discussed in Section~\ref{sec:without}. To this end, we move the imputation step (lines~\ref{craig1:oracle1}-\ref{craig1:oracle} in Algorithm 1) inside the first loop of Algorthm 1, \ie imputing each selected $t^{*}$ by a human or automatic method in each iteration after line~\ref{craig1:add2}.




Afterwards, we will add the next tuple into the coreset, so another loop starts and $h$ tuples are sampled. In the following, we will expand the third loop,   \ie the function \texttt{ComputeUtility} (line~\ref{craig1:loop3})  of Algorithm~\ref{alg:framework} under this one tuple per iteration scenario. 




As shown in Algorithm~\ref{alg:one}, at the beginning, we temporarily add the sampled tuple $t$ to the current coreset, so as to compute the benefit of $t$, \ie  $\mathrm{E}[t|\core]$. To this end, we have to first  compute the expectation  of GA error bound of $\hat{\core}$   (\ie computing $\mathrm{E}[\hat{\core}]$ in the for-loop lines~\ref{one:loop}-\ref{one:dirtysum}). And the expectation \wrt $\core$ (\ie $\mathrm{E}[\core]$) has been computed in the last loop. Then we can compute $\mathrm{E}[t|\core] = \mathrm{E}[\core] - \mathrm{E}[\hat{\core}]$ (line~\ref{one:expt}).


Specifically, to compute $\mathrm{E}[\hat{\core}]$, we will use the tuple-based expectation computation method proposed in Section~\ref{subsec:exp}. For each tuple $t_i\in \train$, if $t_i$ and $t$ are both complete, we can directly compute $\min_{c_j\in \hat{\core}}s_{ij}$ because there is no incomplete data in $\hat{\core}$ (lines~\ref{one:clean}-\ref{one:cleansum}). Otherwise, we will enumerate the possible worlds of $\hat{\core} \cup \{t_i\}$, compute their probabilities and compute $\mathrm{E}[\min_{c_j\in \hat{\core}}s_{ij}]$
 (lines~\ref{one:enumw}-\ref{one:exp4pw}).
  Note that since there are at most two tuples (\ie $t_i$ and $t$) have missing values, the number of possible worlds is small because other missing values in $\hat{\core}$ have been imputed by humans in previous iterations.

\stitle{Time complexity analysis.} As discussed above, using this human-in-the-loop strategy, the number of possible worlds to be considered is greatly reduced. For   Algorithm~\ref{alg:one}, the time complexity is $O(nL^2)$ because there are at most two incomplete tuples in $\hat{\core}$. For the entire three loops framework, the time complexity is  $O(KhnL^2)$, which is much lower  than the solution without imputation in the loop.
  
  However,  if we utilize the human for imputation, the above method  will incorporate many human iterations. In the following,  we propose to ask human to impute a small batch of missing tuples in each iteration, so as to reduce the  number of human iterations.
  

% we initialize the temporary coreset $\core'$ by combining current coreset $\core$ and the sampled tuple $t$ (line~\ref{one:initc}). We also initialize the expectation of $\core'$ as 0 (line~\ref{one:inite}). Then, for each tuple $t_i$ in $\train$, we update the expectation of $t_i$ (line~\ref{one:clean}-\ref{one:dirtysum}). If $t$ and $t_i$ are both complete tuples, we can easily update the expectation without enumerate the possible worlds (line~\ref{one:cleansum}). Otherwise, we first need to enumerate the possible worlds of  $\core' \cup \{t_i\}$ (line~\ref{one:enumw}) and then compute the expectation of $t_i$ in  different possible worlds (line~\ref{one:exp4pw}-\ref{one:dirtysum}). Finally, we can compute the utility of $t$ by comparing the expectation between $\core$ and $\core'$ (line~\ref{one:expt}).



\input{sec/algo-batch}

\subsection{One Batch Each Iteration with Human-in-the-loop}
\label{subsec:batch}

In Section~\ref{subsec:one}, one  tuple per iteration by humans  requires many human iterations. However, if we just incorporate a single human iteration like Section~\ref{subsec:exp}, it is infeasible to compute the tuples to be imputed due to the large number of possible worlds. Therefore, in this subsection, we propose a trade-off solution that asks the human to impute a small batch of tuples per human iteration.

To be specific, as shown in Algorithm~\ref{alg:batch}, compared with the one tuple per human iteration algorithm (\ie the modified Algorithm~\ref{alg:framework} at the beginning of Section ~\ref{subsec:one}), we additionally take the batch size $b$ as input (when $b=1$, Algorithm~\ref{alg:batch} is in fact the modified Algorithm~\ref{alg:framework}). Algorithm~\ref{alg:batch} also incorporates 3 loops, but the main difference is that we do not instantly ask the human to impute the most beneficial tuple $t^*$ among $T_{sample}$. Instead, we just add $t^*$ into the coreset $\core$ (line~\ref{batch:addcoreset}). When there have been $b$ incomplete tuples, we ask the human to impute these tuples together (line~\ref{batch:batchenough}-\ref{batch:batchzero}). Finally we compute the weight (line~\ref{batch:weight}), same as  Algorithm~\ref{alg:framework}.  Although this approach reduces the number of human iterations, it takes a longer time to compute $\mathrm{E}[t|\core]$ (line~\ref{batch:loop3}) than Algorithm~\ref{alg:framework} because there are more incomplete tuples, which indicates more possible worlds. Specifically, the time complexity of computing $\mathrm{E}[t|\core]$ is $O(nL^b)$, which is also expensive. Hence, we propose a heuristic method to accelerate this process as follows. 

\stitle{Reducing the number of possible worlds.} 
A straightforward method of improving the efficiency is to reduce the number of possible worlds. To this end, intuitively, we should focus more on the possible world with a high probability, so these possible worlds with low probabilities can be pruned without sacrificing the accuracy of expectation computation much. Note that for each possible world, the probability is computed by the multiplication of the probabilities of incomplete tuples in the  world because the tuples can be considered independent~\cite{miao2022experimental}. Therefore,  we can remove the possible worlds of each tuple with low probabilities (\ie  reducing $L$), and thus the number of possible worlds of the entire coreset is greatly reduced. 
For example, we can keep top-$l$ (\eg $l=3$) possible worlds (\ie 3 different possible imputations of $t$ with high probabilities) of a tuple $t$. Then for the batch of $b$ incomplete tuples, the number of possible worlds is $l^b$ and the complexity of computing $\mathrm{E}[t|\core]$ is $O(nl^b)$, where both $l$ and $b$ are small enough. Overall, the time complexity is $O(Khnl^b)$. Besides, we can also apply this heuristic method to make the algorithm in Section~\ref{sec:without} practical, which is evaluated in Section~\ref{exp:sec:batchalgo}.






%Recap that in Algorithm~\ref{alg:one}, we need to enumerate the possible worlds of current coreset $\core'$ to compute the benefit of $t$. If the number of possible of each individual tuple (\ie $L$) is large, enumerating all the possible worlds of $\core'$ takes a lot of time since the number is exponential. 

%Then, a straightforward method is to reduce the number of possible worlds of each individual tuple. For example, if every tuple has only 3 possible worlds, the time cost of enumerating the possible worlds for $\core'$ is acceptable. Fortunately, only the possible worlds with high probability of each tuple have a great impact on the computation of benefit. In other words, the possible worlds with low probability could be ignored. Therefore, we propose to use the $top$-$l$ tuples with the largest probability of each tuple when enumerating the possible worlds for $\core'$. To be specific, we only keep the $top$-$l$ tuples with the highest probability for each tuple. Then, only these $top$-$l$ tuples are used in our algorithm. Thus, the complexity of computing $\mathrm{E}[t|\core]$ is $O(nl^b)$, where both $l$ and $b$ are small enough.




















