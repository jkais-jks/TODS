%!TEX root = ../main.tex
\subsection{Experimental Settings}

\noindent{\bf Dataset.} We evaluate on 6 real-world datasets that are often used in the field of data imputation~\cite{DBLP:conf/icde/LiRBZCZ21, liu2021adaptive, DBLP:journals/pvldb/KrishnanWWFG16,DBLP:journals/pvldb/KarlasLWGC0020}, as shown in Table~\ref{tbl:dataset}, where $M$ denotes the number of attributes.   
%They cover different downstream ML tasks, including binary classification, multi-classification and regression.  

%Among these datasets, there are not enough missing values in the some of them. Thus, we manually create some incomplete tuples for them. We can also obtain the ground truth of these manually created missing values to better compare different baselines. 
%In addition, numerical attribute is hard to impute, thus, we convert them to categorical attribute using bucketing technique. The details of these datasets are as below.

\noindent{\bf (1) \nursery}~\cite{nursery} is a multi-classification task, which  predicts ``{\it the level of recommendation for whether a child goes to school}''. There are five different levels, \ie $\{ {\tt not\_recom, priority, recommend, spec\_prior, very\_recom} \}$. %There are 9 categorical attributes. %The size of train, validation and test are 10960, 1000 and 1000. We manually created 3000 incomplete tuples in the train dataset.
{\bf (2) \hr}~\cite{chai2022selective} is a binary classification task of ``{\it predicting whether an employee would change the job}''.  %There are 12 attributes, one of which  is numerical. %and we convert it to categorical attribute using bucketing technique. The size of train, validation and test data are 18287, 2000 and 2000. In the train dataset, there are 5000 incomplete tuples.
{\bf (3) \adult}~\cite{adult} is a binary classification task that predicts ``{\it if the annual revenue of a people is over 50000 dollars}''. %There are 14 attributes in this dataset, 6 of which are categorical. %The size of train, validation and test dataset are 32842, 8000 and 8000. In the train dataset, there are 12500 incomplete tuples. 
{\bf (4) \credit}~\cite{kaggle} is a binary classification task that predicts ``{\it whether the loan will be deferred based on a person's economic situation}''.  %The size of train, validation and test dataset are 131,000, 4000 and 4000. In the train dataset, there are 87000 incomplete tuples.
{\bf (5) \bike}~\cite{bike} is a regression task that predicts ``{\it the number of bike sharing in a given time}''. %There are 15 attributes in this dataset, 5 of which are numerical data. %The size of train, validation and test dataset are 13300, 2000 and 2000. In the train dataset, there are 5100 incomplete tuples.
{\bf (6) \air}~\cite{auctus} is a regression task that predicts ``{\it the air quality at a certain time}''. %There are 18 attributes in this dataset, 12 of which are numerical data.
{\bf (7) \imdb}~\cite{DBLP:journals/pvldb/LeisGMBK015} refers to a dataset that \textit{predicts the rating (1-10) of movies}, which contains the basic information of movies, \eg \texttt{movie\_id, title, production\_year}.

{\bf (8) \imdbl}~\cite{DBLP:journals/pvldb/LeisGMBK015} is the large vision of \imdb, which contains 4000000 records with the same attributes.

%Table~\ref{xxx} shows the statistics of these datasets.

For datasets (1)-(3) and (7)-(8), we follow existing works~\cite{DBLP:journals/jbd/KhanH20,DBLP:conf/icml/YoonJS18,DBLP:conf/mlsys/WuZIR20} to manually inject missing values until the  rate of missing tuples is 30\%, and we will vary the percentage of incomplete tuples in Section~\ref{exp:sec:sensitivity}. Datasets (4)-(6) already contain missing values. For all datasets, we randomly split them for 80\%/10\%/10\% as train/validation/test sets.






\noindent{\bf Evaluation metrics.}
We mainly evaluate the effectiveness and efficiency of \ours and  baselines. For effectiveness, we use the {\it prediction accuracy} for the classification task and use the {\it mean square error} ($MSE = \frac{\sum_{i=1}^N(y_i-\hat{y}_i)}{N}$, where $N$ denotes the size of test set) for the regression task. 

For efficiency, we focus on the machine cost (\ie the runtime of machine) as well as the human cost (the number of tuples imputed by human for human-involved methods). For datasets (1)-(3) and (7)-(8), we have the ground truth of missing tuples, so we use them to simulate the human imputation. For datasets (4)-(6), we leverage the expert to impute missing values in the coreset by looking at the top-5 values recommended by the automatic method as a reference. Note that we only involve humans when it is affordable. For baselines that require humans to impute a lot of missing tuples (\ie~\truth~and $\one$ as below), we will not apply them on datasets (4)-(6).

\noindent{\bf Baselines.} We compare \ours and \ours$^+$ with a variety of baselines.

%\noindent{\bf (1) \noclean} refers to just training on $\train$. \cc{necessary?}



%\noindent{\bf (3) \truthcore} first uses the ground truth to impute the missing values and then selects the coreset for the cleaned dataset. Similar to \truth, it only works for the datasets with manually created missing values. 

%\noindent{\bf (3) \mice} is a typical missing value imputation approach. \mice uses linear regression to estimate the missing values. \mice conducts imputation for a missing value several times and uses the average value as the final result.

%\noindent{\bf (5) \missf} is a tree-based imputation method. It recasts data imputation as a prediction task and uses random forest to predict the missing values.

%Data are imputed by regressing each variable in turn against all other variables and then predicting missing data for the dependent variable using the fitted forest.
\begin{table}
	\centering
	\caption{Statistics of datasets}
	\vspace{-1em}
	{\small
	\begin{tabular}{ccccc}
		\hline
		{\bf Dataset} & {\bf $|\train|$} & {\bf $m$} & {\bf $\#$ Incomp. Tuples} & {\bf Task}\\
		\hline	
		\nursery & 	10960 & 9 & 3218 & Multi-Class. \\
		\hr & 18287 & 12 & 5475 & Binary Class. \\
		\adult & 32842 & 14 & 10752 & Binary Class. \\
		\credit & 131,000 & 11 & 76813 & Binary Class. \\
		\bike & 13300 & 15 & 4821 & Regression \\
		\air & 437,200  & 18 & 128,372 & Regression\\
		\imdb & 1,000,000 & 40 & 331,189 & Multi-Class.\\
		\imdbl & 4,000,000 & 40 & 1,312,908 & Multi-Class.\\
		\hline
	\end{tabular}
	}
	\label{tbl:dataset}
%	\vspace{-1.8em}
\end{table}

\noindent{\bf (1) \noclean}~refers to just training on $\train$.

\noindent{\bf (2) \actclean}~\cite{DBLP:journals/pvldb/KrishnanWWFG16} is an iterative data cleaning framework, which estimates the impact of tuples  and prioritizes cleaning the tuples that much affect the model performance. In each iteration, it can ask the human to clean a sample subset of tuples. We set the sample size to 50, same as the paper. 

\noindent{\bf (3) \boostclean}~\cite{DBLP:journals/corr/abs-1711-01299} is an automatic  data cleaning method that iteratively selects a cleaning method from several pre-defined algorithms, applies to the train dataset and  updates the model.  We use MICE~\cite{royston2011multiple}, MISSForest~\cite{DBLP:journals/bioinformatics/StekhovenB12}, GAIN~\cite{DBLP:conf/icml/YoonJS18} as pre-defined algorithms.


\noindent{\bf (4) \gain}  uses MICE~\cite{royston2011multiple}, MISSForest~\cite{DBLP:journals/bioinformatics/StekhovenB12}, GAIN~\cite{DBLP:conf/icml/YoonJS18} to respectively impute the train set and selects the one that achieves the highest accuracy on the validation set.

\noindent{\bf (5) \truth} is an ideal case that trains on the ground truth, \ie $\trainc$. Note that only datasets (1)-(3) have the ground truth to evaluate this baseline.  Datasets (4)-(6) do not have the ground truth and it is  too expensive to ask the human to impute so many missing values.

\noindent{\bf (6) \mixcore} is a baseline that selects a coreset from all complete tuples, and then we randomly select some incomplete tuples to impute. We set the number of incomplete tuples to be imputed equal to that of  other baselines for fair comparison. Finally we train with the tuples in the coreset plus the  imputed ones.



%It treats the data cleaning as a minimax optimization problem. The generator is optimized to impute the incomplete tuples and makes them as similar as possible to the complete tuples. While the discriminator is utilized to distinguish the imputed tuples. Thus, the incomplete tuples can be imputed during this minimax game.

%\noindent{\bf (7) \rand} randomly samples a subset from the dirty dataset $\train$ and imputes the missing values in it. Then, it trains a downstream model on this subset.


\noindent{\bf (7) $\one$} first involves human to impute the dataset $\train$ and then selects a coreset. Similar to \truth, only datasets (1)-(3) can be evaluated on it because they have the ground truth. The coreset selection solution is the algorithm in~\cite{DBLP:conf/icml/MirzasoleimanBL20}, which is a greedy algorithm by modifying Algorithm 1 without considering the possible worlds.


\noindent{\bf (8)  $\two$} first uses automatic data imputation methods to impute the dataset $\train$, and then selects a coreset using the same method of baseline (7).

\noindent{\bf (9) $\three$} directly selects a coreset based on $\train$ and then asks human to impute the incomplete tuples of the coreset.

\noindent{\bf (10) $\four$} also directly selects a coreset from $\train$, it then uses MICE~\cite{royston2011multiple} to impute the incomplete tuples in the coreset.

\noindent{\bf Our solutions.} We compare \ours and its variants.

\noindent{\bf (11) $\seven$} uses \ours to select the coreset and iteratively asks human to impute incomplete tuples (one tuple per human iteration) during the coreset selection process.

\noindent{\bf (12) $\eight$} is similar to $\goodfunc(\train,\circlearrowleft^\humanfunc)$, but the automatic  MICE  method is used.

\noindent{\bf (13) $\nine$} uses group-based method to accelerate \ours, which select the coreset and iteratively asks human to impute incomplete tuples (one tuple per human iteration) during the coreset selection process.

\noindent{\bf (14) $\ten$} is similar to $\nine$, but the automatic  MICE  method is used.


Besides, since the coreset of $\five$ (or $\six$) is too expensive to compute due to the large number of possible worlds, we do not directly compare with it. Instead, we will limit the number of possible worlds of each tuple to 3 as discussed in Section~\ref{subsec:batch} and evaluate in Section~\ref{exp:sec:batchalgo}.

%\noindent{\bf (12) $\five$} first uses \ours to select a coreset and then involves human to impute this coreset. Since the machine cost is too high, we limit the number of possible worlds of each tuple to 3 and evaluate in Section~\ref{exp:sec:batchalgo}.

%\noindent{\bf (13) $\six$} also uses \ours to select a coreset and leverages MICE to impute this coreset. For the same reason as $\five$, we also  limit $l=3$  and evaluate in Section~\ref{exp:sec:batchalgo}.

 



%\noindent{\bf (10) \autocore} first uses an automatic data imputation method (\eg MICE) to impute the dirty dataset $\train$. Then, it selects a coreset of the cleaned dataset and involves human to review the missing values in the coreset.




\noindent{\bf Hyper-parameter setting.}
%For classification tasks, we use SVM as the default downstream model. For regression tasks, we use linear regression as the default.
We use SVM and linear regression as the default downstream model for classification and regression tasks, respectively. We vary the downstream models in Section~\ref{exp:sec:sensitivity}. For model training, we use SGD and k-inverse decay  scheduling, \ie $\alpha_k = \alpha_0 / (1+bk)$ ($\alpha_0$ and $b$ are hyper-parameters to be tuned independently for different methods).  The sample size $h$ is set to 200 as default and we vary the  size in Section~\ref{exp:sec:sensitivity}. The number of training epochs is set as 20. %\cc{
We also impute the test data using the same method that is applied to the train data before testing.
%}