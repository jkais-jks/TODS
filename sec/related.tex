%!TEX root = ../main.tex
%\vspace{-1em}
\section{Related Work}\label{sec:related}

\noindent{\bf Task-agnostic incomplete data imputation.}
Data imputation has been widely studied for  years. Existing  methods can be divided into two categories: statistic-based methods and learning-based methods. The former one always uses the statistic information~\cite{DBLP:journals/tsmc/FarhangfarKP07, DBLP:conf/sigmod/MayfieldNP10} (like mean, median or mode) to impute the missing values. Also, some methods compute the similarity of the incomplete tuples to the complete tuples and use the most similar one to impute the missing values~\cite{altman1992introduction, DBLP:journals/artmed/JerezMGARMF10, DBLP:conf/isese/TwalaCS05}. Recently, to improve the imputation accuracy,   many learning-based methods focus on how to use ML to learn the data distribution (\eg MissForest imputation~\cite{DBLP:journals/bioinformatics/StekhovenB12}, MICE~\cite{royston2011multiple},  IIM~\cite{DBLP:conf/icde/ZhangSSW19}), and then  use the trained model to predict the missing values. Besides traditional ML models, some deep learning models are also used for data imputation (\eg  autoencoder~\cite{DBLP:journals/corr/GondaraW17, mccoy2018variational, DBLP:journals/pr/NazabalOGV20}, GANs~\cite{DBLP:journals/nn/SpinelliSU20, DBLP:conf/icml/YoonJS18}).
 
\noindent{\bf Coreset selection.} A previous work~\cite{goodcore} has studied how to select a well-performed coreset over incomplete tuples. The extension to the previous study in this work is four-fold. First, we propose a new framework that incorporates the group-based strategy into the coreset selection process with incomplete data. Secondly, we theoretically analyze that the group-based strategy can still have a bounded GA error. Third, we propose an efficient solution to estimate the upper bound, and several strategies to reduce the number of possible worlds. Fourth, two large datasets are added and multiple experiments are conducted to demonstrate the efficacy of our proposed methods. 

%borsos2020coresets, killamsetty2021retrieve
%
Besides, 
  Huang et al.~\cite{DBLP:conf/icml/HuangHLFD21} studied how to compute and continuously update the coreset while training, %, which aims  to utilize the loss of training tuples  to approximate the overall training loss of the entire training set. 
but it is rather time-consuming because of the training process. 
 To solve this problem, works~\cite{DBLP:conf/icml/CampbellB18, DBLP:journals/corr/BravermanFL16}   selected the coreset without training in advance, but they can only be customized to particular model types respectively. %The basic idea is to compute a sensitive score for each tuple, and the higher the score, the more likely the tuple should be in the coreset. 
 Some works~\cite{sener2017active,chen2012super, Zhang2019SmallGAN, Agarwal2020Context} study how to select a subset of data only considering the data distribution rather than   the model performance.
 Gradient-based methods~\cite{DBLP:conf/icml/MirzasoleimanBL20,  recon, fastcore, DBLP:conf/emnlp/KirchhoffB14, Mirzasoleiman2020Coresetnoisy} focused on selecting the coreset to approximate the full gradient without training in advance for multiple model types, which is regarded as an optimization problem as discussed in Section~\ref{subset:sigletable}.
 Coreset selection can also be formulated as a bilevel optimization problem~\cite{borsos2020coresets, killamsetty2021retrieve, Killamsetty2022GLISTER}, where the outer objective involves choosing a subset and the inner objective entails optimizing model parameters on the subset, the ultimate goal is to select a subset such that the model empirical risk on this subset  approximates that of the model trained on the complete dataset. However it is rather time-consuming because it requires solving an inner optimization problem during each outer iteration. 
 
In short, none of the above methods except~\cite{goodcore}  consider coreset selection over incomplete data.

% Coreset selection can be formulated as a bilevel optimization problem, where the outer objective involves choosing a subset and the inner objective entails optimizing model parameters on the subset, the ultimate goal is to select a subset such that the model's empirical risk on this subset closely approximates that of the model trained on the complete dataset. But it is time consuming because it requires solving an inner optimization problem during each outer iteration. 

\noindent{\bf Data cleaning for ML.} Recently, there have  been several works that clean the data to optimize the ML model. 
In contrast to the above discussion about task-agnostic incomplete data imputation, data cleaning for ML is task-aware, which triggers new technical challenges.
SampleClean~\cite{DBLP:journals/debu/KrishnanWFGKM015} focuses on cleaning selected samples, so as to answer  SQL aggregate  queries  more efficiently, but it is not for any model.
%
 CPClean~\cite{DBLP:journals/pvldb/KarlasLWGC0020} proposes certain prediction to impute missing data for optimizing ML models. Different from us, it is customized to nearest neighbor classifiers rather than convex models solved by the gradient descent algorithm. 
  %
  BoostClean~\cite{DBLP:journals/corr/abs-1711-01299} regards data cleaning as a boosting problem that iteratively
 selects from a predefined set of cleaning algorithms, so as to continuously maximize the accuracy of  a validation set with training iteratively. 
Closer to our work is ActiveClean~\cite{DBLP:journals/pvldb/KrishnanWWFG16}, which progressively cleans the data tuples that are likely to much influence the model   measured by the gradients. 
%\cc{
Different from us,  given a budget $K$, we can select the coreset without training, but ActiveClean needs to train iteratively  and label a set of   validation  dataset.  We empirically show that our method outperforms ActiveClean on model accuracy and efficiency in Section~\ref{sec:exp}.
%}.
%(2) ActiveClean uses heuristics to measure the influence of tuples to the model gradients, which may not be accurate.
% Moreover, we  ActiveClean to the data imputation problem and compare with it in Section~\ref{sec:exp}.}

%ActiveClean iteratively estimates the impact of the dirty data and samples a batch of dirty data that have the most impact on the model. Then, ActiveClean asks human to clean the batch of dirty data. The main difference between ActiveClean and \ours is ActiveClean needs to iteratively train the model, since it relies on the feedback from both human and model to estimate the impact of the dirty data.


%\noindent{\bf Data Preparation for AI.}
%Data preparation can be utilized to improve the effectiveness of ML model, including data discovery~\cite{liu2021automatic, chai2022selective, liu2022feature},  data cleaning~\cite{hao2020outdated,chai2020human}, data labeling~\cite{chai2016cost, li2018cdb}, data cleaning~\cite{miao2022experimental, gao2018query,miao2018incomplete, miao2021efficient,wu2022interactive,ge2020hybrid} and data exploration~\cite{qin2020interactively,qin2021ranking}.