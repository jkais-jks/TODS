%!TEX root = ../main.tex
\vspace{-0.5em}
\subsection{Convergence Evaluation}

In Section~\ref{sec:proof}, we have shown  the convergence rate of \ours theoretically. In this part, we test the convergence of training over the coreset ($\seven$) and entire data (\truth) empirically.  Figure~\ref{fig:converge} shows the test accuracy of two methods with the number of training iterations increasing. We can observe that on both datasets, training on the coreset converges much faster than training on the full data. 
For example, on dataset \adult, it takes $\sim$40 iterations for \ours to converge, which is  $180\times$ faster than {\truth}. This is because \ours has the same convergence rate with training over the entire dataset as discussed in the theoretical result of Section~\ref{sec:proof}, but the entire dataset (\eg  ~\adult) is $200\times$ (similar to $180\times$) larger than the coreset ($\rho=0.005$).
 That is, \ours converges with the same number of epochs  as training on the entire dataset. Since the size of coreset is much smaller, \ours is more efficient. Also, we can achieve competitive accuracy as training on full data by approximating the full gradient with a theoretical bound. 
 

%\add{Figure~\ref{fig:real_loss} also shows the cross entropy loss of two methods with the number of iterations increasing. We can obtain the actual convergence rate from Figure~\ref{fig:real_loss} and compare it with the theoretical value. For example, at the $1^{st}$, $5^{th}$, $10^{th}$ epoch, we find that the actual convergence rates are 6.77, 3.25 and 1.26. The theoretical convergence rates are 8.4, 3.8 and 2.7.}

Furthermore, we report the loss change to reflect the relation between  actual convergence rate and theoretical results. In Figure~\ref{fig:real_loss}, on dataset \adult, the initial loss is 8.4. According to the theoretical convergence rate $O(\frac{1}{\sqrt{k}})$ (this $k$ denotes the $k$-th epoch),  the loss should decrease to around 3.8 at the end of 5-th epoch ($\approx$ 3200-th iteration). Actually,  the  actual loss decreases to 3.25 at that time, which is close to the theoretical value.

 %Thus, \ours needs the same number of epochs to converge as training on the entire dataset. Since the size of coreset generated by \ours is much smaller than the entire dataset, training on the coreset needs much smaller number of iterations to converge.  \cc{proves what? the result shows what? }In this part, we evaluate the convergence of \ours.  Figure~\ref{fig:converge} shows the performance on test dataset for $\seven$ and {\truth} with increasing SGD iterations. \cc{what does the iteration mean? epoch??} {\truth} reflects the convergence on the whole train dataset. We can see that training on the coreset select by \ours is much faster than on the whole train dataset. On dataset \adult, it take about 60 iterations for \ours to converge, which is nearly $120\times$ faster than {\truth}. Besides, the performance of our method is competitive to {\truth}. That is to say, we can make the training process more efficient and without scarifying model performance. 