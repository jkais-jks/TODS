\documentclass[acmsmall]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%% 隐藏acmref和copyright
% \settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
% \pagestyle{plain}


%%
%% These commands are for a JOURNAL article.
% \acmJournal{JACM}
% \acmVolume{37}
% \acmNumber{4}
% \acmArticle{111}
% \acmMonth{8}

\input{sec/commands}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{GoodCore: Data-effective and Data-efficient Machine Learning through Coreset Selection over Incomplete Data}

\author{ANONYMOUS AUTHOR(S)}
\authorsaddresses{}


\begin{abstract}
Given a dataset with incomplete data (e.g., missing values), training a machine learning model over the incomplete data requires two steps. First,  it requires a data-effective step that cleans the data in order to improve the data quality (and the model quality on the cleaned data). Second, it requires a data-efficient step that selects a core subset of the data (called coreset) such that the trained models on the entire data and the coreset have similar model quality, in order to improve the training efficiency. The first-data-effective-then-data-efficient methods are too costly, because they are expensive to clean the whole data; while the first-data-efficient-then-data-effective methods have low model quality, because they cannot select high-quality coreset for incomplete data.
	
In this paper, we investigate the problem of coreset selection over incomplete data for data-effective and data-efficient machine learning. The essential challenge is how to model the incomplete data for selecting high-quality coreset.  To this end, we propose the \ours framework towards selecting a good coreset over incomplete data with low cost. To model the unknown complete data, we utilize the combinations of possible repairs as possible worlds of the incomplete data. 	Based on possible worlds, \ours  selects an expected optimal coreset through gradient approximation without training ML models. We formally define the expected optimal coreset selection problem, prove its NP-hardness, and propose a greedy algorithm with an approximation ratio. To make \ours more efficient, we further propose optimization methods that incorporate human-in-the-loop imputation or automatic imputation method into our framework. Experimental results show the effectiveness and efficiency of our framework with low cost.
\end{abstract}

\begin{CCSXML}
	<ccs2012>
	   <concept>
		   <concept_id>10010147.10010257</concept_id>
		   <concept_desc>Computing methodologies~Machine learning</concept_desc>
		   <concept_significance>500</concept_significance>
		   </concept>
	   <concept>
		   <concept_id>10010147.10010257.10010293</concept_id>
		   <concept_desc>Computing methodologies~Machine learning approaches</concept_desc>
		   <concept_significance>300</concept_significance>
		   </concept>
	   <concept>
		   <concept_id>10010147.10010257.10010321</concept_id>
		   <concept_desc>Computing methodologies~Machine learning algorithms</concept_desc>
		   <concept_significance>300</concept_significance>
		   </concept>
	   <concept>
		   <concept_id>10002951.10002952.10003219.10003218</concept_id>
		   <concept_desc>Information systems~Data cleaning</concept_desc>
		   <concept_significance>500</concept_significance>
		   </concept>
	 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[300]{Computing methodologies~Machine learning approaches}
\ccsdesc[300]{Computing methodologies~Machine learning algorithms}
\ccsdesc[500]{Information systems~Data cleaning}

\keywords{data-centric AI; machine learning; data cleaning; coreset selection}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

\settopmatter{printfolios=true} 
\maketitle

\input{sec/intro}
\input{sec/00-background}
\input{sec/00-problem}
\input{sec/00-method}
\input{sec/00-human}
\input{sec/00-convrate}
\input{sec/cluster}
\input{sec/00-exp}
\input{sec/related}
\input{sec/00-conclusion}
	
%\balance
\clearpage

%\bibliographystyle{abbrv}
\normalem
\bibliographystyle{ACM-Reference-Format}
\bibliography{DA}

\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
